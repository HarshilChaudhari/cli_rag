
# ðŸ“š CLI RAG â€“ PDF Question Answering with Gemini & Ollama

A lightweight **Retrieval-Augmented Generation (RAG) system** that lets you load PDFs, embed them into a FAISS vector store, and query them interactively using either **Google Gemini** (cloud) or **Ollama + Mistral/Gemma** (local).

---

## âœ¨ Features
- Load and chunk PDFs (PyMuPDF â†’ OCR fallback with Tesseract).
- Store embeddings in SQLite + FAISS.
- Query interactively via CLI.
- Switch between **Gemini API** or **Ollama local LLM** with one environment variable.
- Skips already-processed PDFs (no duplicate work).
- Minimal dependencies & easy to extend.

---

## ðŸ“‚ Project Structure
```
cli_rag/
â”‚â”€â”€ __init__.py
â”‚â”€â”€ agent.py        # RAG agent logic (retrieval + LLM)
â”‚â”€â”€ cli.py          # Main CLI entrypoint
â”‚â”€â”€ config.py       # Global config (models, API keys, defaults)
â”‚â”€â”€ llm.py          # LLM backends: Gemini / Ollama
â”‚â”€â”€ pdf_loader.py   # PDF â†’ chunks (PyMuPDF â†’ OCR fallback)
â”‚â”€â”€ retrieval.py    # Hybrid retrieval + answer synthesis
â”‚â”€â”€ store.py        # SQLite + FAISS vector store (persistence)
â”‚â”€â”€ memory.py       # (optional) conversational memory helpers
â”‚
â”œâ”€ generated at runtime (repo root):
â”‚   â”œâ”€ rag_store.db        # SQLite DB
â”‚   â””â”€ rag_index.faiss     # FAISS index

````

---

## âš¡ Setup

### 1. Clone repository
```bash
git clone https://github.com/<your-username>/<your-repo>.git
cd <your-repo>
````

### 2. Install dependencies

```bash
pip install -q langgraph langchain langchain-community google-generativeai langchain-google-genai pypdf PyPDF2 sentence-transformers scikit-learn faiss-cpu

```

Typical requirements:

```txt
PyMuPDF
pdf2image
pytesseract
PyPDF2
faiss-cpu
sentence-transformers
google-generativeai
requests
```

(plus `poppler` and `tesseract-ocr` system packages if OCR is needed)

### 3. Configure API keys

Set your **Gemini API key** (if using Gemini):

```bash
export GEMINI_API_KEY="your_api_key_here"
```

### 4. (Optional) Run Ollama locally

Install [Ollama](https://ollama.ai/) and pull a model, e.g.:

```bash
ollama pull mistral
```

---

## ðŸš€ Usage

### Run with PDFs

```bash
python -m cli_rag.cli /path/to/paper.pdf /path/to/book.pdf
```

### Run without new PDFs (use existing DB)

```bash
python -m cli_rag.cli
```

### Example session

```
Loading PDFs into database...
ðŸ“„ Added /content/NIPS-2017-attention-is-all-you-need-Paper.pdf (doc_id=51bfe776f7a7, 11 chunks)

Interactive RAG CLI. Type 'exit' to quit.

> What is the key contribution of "Attention is All You Need"?
...
```

---

## ðŸ”€ Switching Backends

Choose between **Gemini** or **Ollama** by setting `LLM_BACKEND`:

```bash
# Use Gemini (default)
export LLM_BACKEND=gemini

# Use Ollama (local)
export LLM_BACKEND=ollama
```

---

## ðŸ› ï¸ Development

### Branching

* `main` â†’ stable (Gemini by default)
* `local-llm` â†’ experiments with Ollama / Mistral

### Update dependencies

```bash
pip freeze > requirements.txt
```

---

## ðŸ“Œ Roadmap

* [ ] Add multi-PDF querying (combine results from multiple docs).
* [ ] Add web UI (Streamlit / FastAPI).
* [ ] Improve OCR pipeline (parallel processing).
* [ ] Support more LLMs (Claude, LLaMA, OpenAI GPT).

---


